{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfc5597",
   "metadata": {},
   "source": [
    "# Assignment 8 - (Data Warehousing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8151045c",
   "metadata": {},
   "source": [
    "\n",
    "TOPIC: Data Warehousing Fundamentals\n",
    "   1. Design a data warehouse schema for a retail company that includes dimension tables for products, customers, and time. Implement the schema using a relational database management system (RDBMS) of your choice.\n",
    "   2. Create a fact table that captures sales data, including product ID, customer ID, date, and sales amount. Populate the fact table with sample data.\n",
    "   3. Write SQL queries to retrieve sales data from the data warehouse, including aggregations and filtering based on different dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b70c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create the Sales Fact Table\n",
    "CREATE TABLE sales_fact (\n",
    "    sales_id INT PRIMARY KEY,\n",
    "    product_id INT,\n",
    "    customer_id INT,\n",
    "    date_id INT,\n",
    "    sales_amount DECIMAL(10,2),\n",
    "    FOREIGN KEY (product_id) REFERENCES products_dim (product_id),\n",
    "    FOREIGN KEY (customer_id) REFERENCES customers_dim (customer_id),\n",
    "    FOREIGN KEY (date_id) REFERENCES time_dim (date_id)\n",
    ");\n",
    "\n",
    "\n",
    "-- Populate the Sales Fact Table with Sample Data\n",
    "INSERT INTO sales_fact (sales_id, product_id, customer_id, date_id, sales_amount)\n",
    "VALUES\n",
    "    (1, 1, 1, 1, 100.50),\n",
    "    (2, 2, 1, 1, 50.25),\n",
    "    (3, 3, 2, 2, 75.80),\n",
    "    -- Additional sample data\n",
    "    (4, 1, 3, 3, 200.00),\n",
    "    (5, 4, 4, 4, 150.75);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE products_dim (\n",
    "    product_id INT PRIMARY KEY,\n",
    "    product_name VARCHAR(100),\n",
    "    category VARCHAR(50),\n",
    "    brand VARCHAR(50),\n",
    "    price DECIMAL(10,2)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc2da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE customers_dim (\n",
    "    customer_id INT PRIMARY KEY,\n",
    "    customer_name VARCHAR(100),\n",
    "    address VARCHAR(100),\n",
    "    city VARCHAR(50),\n",
    "    state VARCHAR(50),\n",
    "    country VARCHAR(50)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT p.product_id, p.product_name, SUM(sf.sales_amount) AS total_sales_amount\n",
    "FROM sales_fact sf\n",
    "JOIN products_dim p ON sf.product_id = p.product_id\n",
    "GROUP BY p.product_id, p.product_name;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ad695",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT t.year, t.month, SUM(sf.sales_amount) AS total_sales_amount\n",
    "FROM sales_fact sf\n",
    "JOIN time_dim t ON sf.date_id = t.date_id\n",
    "GROUP BY t.year, t.month;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98940e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT p.product_name, t.year, t.month, sf.sales_amount\n",
    "FROM sales_fact sf\n",
    "JOIN products_dim p ON sf.product_id = p.product_id\n",
    "JOIN time_dim t ON sf.date_id = t.date_id\n",
    "WHERE p.product_name = 'Product A' AND t.year = 2022;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c81ec",
   "metadata": {},
   "source": [
    "TOPIC: ETL and Data Integration\n",
    "  1. Design an ETL process using a programming language (e.g., Python) to extract data from a source system (e.g., CSV files), transform it by applying certain business rules or calculations, and load it into a data warehouse.\n",
    "  2. Implement the ETL process by writing code that performs the extraction, transformation, and loading steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf45b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import psycopg2  # Assuming PostgreSQL as the data warehouse\n",
    "\n",
    "# Extraction\n",
    "source_file = \"path/to/source_file.csv\"\n",
    "\n",
    "# Transformation\n",
    "def transform_data(row):\n",
    "    # Apply transformations or business rules to each row of data\n",
    "    # Example: Perform calculations or manipulate values\n",
    "    transformed_row = {\n",
    "        \"column1\": row[\"column1\"],\n",
    "        \"column2\": row[\"column2\"],\n",
    "        # Apply transformations to other columns\n",
    "    }\n",
    "    return transformed_row\n",
    "\n",
    "# Loading\n",
    "def load_data_to_warehouse(data):\n",
    "    # Connect to the data warehouse\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"mydatawarehouse\",\n",
    "        user=\"myuser\",\n",
    "        password=\"mypassword\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create the target table in the data warehouse if needed\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS target_table (\n",
    "        column1 datatype,\n",
    "        column2 datatype,\n",
    "        -- Add other columns as needed\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Load the transformed data into the data warehouse\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO target_table (column1, column2)\n",
    "    VALUES (%s, %s);\n",
    "    \"\"\"\n",
    "    for row in data:\n",
    "        values = (row[\"column1\"], row[\"column2\"])\n",
    "        cursor.execute(insert_query, values)\n",
    "    conn.commit()\n",
    "\n",
    "    # Close the connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Perform ETL process\n",
    "data = []\n",
    "\n",
    "# Extraction\n",
    "with open(source_file, \"r\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        data.append(transform_data(row))\n",
    "\n",
    "# Loading\n",
    "load_data_to_warehouse(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0bd053",
   "metadata": {},
   "source": [
    "TOPIC: Dimensional Modeling and Schemas\n",
    "   1. Design a star schema for a university database, including a fact table for student enrollments and dimension tables for students, courses, and time. Implement the schema using a database of your choice.\n",
    "   2. Write SQL queries to retrieve data from the star schema, including aggregations and joins between the fact table and dimension tables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "\n",
    "# Establish a connection to Hive\n",
    "conn = hive.Connection(host='localhost', port=10000, username='your_username')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# HiveQL query to create the partitioned table\n",
    "create_table_query = \"\"\"\n",
    "    CREATE TABLE Sales_Data (\n",
    "        id INT,\n",
    "        product STRING,\n",
    "        price FLOAT\n",
    "    )\n",
    "    PARTITIONED BY (year INT, month INT)\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the table\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "# HiveQL query to add partitions to the table\n",
    "add_partitions_query = \"\"\"\n",
    "    ALTER TABLE Sales_Data\n",
    "    ADD PARTITION (year=2022, month=1)\n",
    "    ADD PARTITION (year=2022, month=2)\n",
    "    ADD PARTITION (year=2022, month=3)\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to add partitions\n",
    "cursor.execute(add_partitions_query)\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69862497",
   "metadata": {},
   "source": [
    "TOPIC: Dimensional Modeling and Schemas\n",
    "   1. Design a star schema for a university database, including a fact table for student enrollments and dimension tables for students, courses, and time. Implement the schema using a database of your choice.\n",
    "   2. Write SQL queries to retrieve data from the star schema, including aggregations and joins between the fact table and dimension tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af710f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE students_dim (\n",
    "    student_id INT PRIMARY KEY,\n",
    "    student_name VARCHAR(100),\n",
    "    student_major VARCHAR(50),\n",
    "    student_level VARCHAR(50)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0196afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE courses_dim (\n",
    "    course_id INT PRIMARY KEY,\n",
    "    course_name VARCHAR(100),\n",
    "    course_department VARCHAR(50),\n",
    "    course_instructor VARCHAR(100)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE time_dim (\n",
    "    date_id INT PRIMARY KEY,\n",
    "    date DATE,\n",
    "    day INT,\n",
    "    month INT,\n",
    "    year INT\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ddff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE enrollments_fact (\n",
    "    enrollment_id INT PRIMARY KEY,\n",
    "    student_id INT,\n",
    "    course_id INT,\n",
    "    date_id INT,\n",
    "    grade VARCHAR(2),\n",
    "    FOREIGN KEY (student_id) REFERENCES students_dim (student_id),\n",
    "    FOREIGN KEY (course_id) REFERENCES courses_dim (course_id),\n",
    "    FOREIGN KEY (date_id) REFERENCES time_dim (date_id)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT c.course_department, COUNT(*) AS total_enrollments\n",
    "FROM enrollments_fact e\n",
    "JOIN courses_dim c ON e.course_id = c.course_id\n",
    "GROUP BY c.course_department;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b6829",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT s.student_name, e.grade\n",
    "FROM enrollments_fact e\n",
    "JOIN students_dim s ON e.student_id = s.student_id\n",
    "JOIN time_dim t ON e.date_id = t.date_id\n",
    "JOIN courses_dim c ON e.course_id = c.course_id\n",
    "WHERE c.course_name = 'Introduction to Computer Science' AND t.semester = 'Fall 2022';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fbb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT s.student_major, AVG(e.grade) AS average_grade\n",
    "FROM enrollments_fact e\n",
    "JOIN students_dim s ON e.student_id = s.student_id\n",
    "GROUP BY s.student_major;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b63456",
   "metadata": {},
   "source": [
    "TOPIC: Performance Optimization and Querying\n",
    "    1. Scenario: You need to improve the performance of your data loading process in the data warehouse. Write a Python script that implements the following optimizations:\n",
    "Utilize batch processing techniques to load data in bulk instead of individual row insertion.\n",
    "      b)  Implement multi-threading or multiprocessing to parallelize the data loading process.\n",
    "      c)  Measure the time taken to load a specific amount of data before and after implementing these optimizations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9727432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "# Function to simulate data loading for a single row\n",
    "def load_data(row):\n",
    "    # Perform data loading operations for a single row\n",
    "    time.sleep(0.1)  # Simulating data loading time\n",
    "\n",
    "# Function to load data in batches\n",
    "def load_data_batch(data_batch):\n",
    "    for row in data_batch:\n",
    "        load_data(row)\n",
    "\n",
    "# Function to measure time taken for data loading\n",
    "def measure_loading_time(data):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data using batch processing\n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        data_batch = data[i:i + batch_size]\n",
    "        load_data_batch(data_batch)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    loading_time = end_time - start_time\n",
    "    return loading_time\n",
    "\n",
    "# Function to load data using multi-threading\n",
    "def load_data_multithread(data):\n",
    "    num_threads = 4  # Number of threads to use\n",
    "\n",
    "    def worker(data_batch):\n",
    "        load_data_batch(data_batch)\n",
    "\n",
    "    threads = []\n",
    "    for i in range(0, len(data), len(data) // num_threads):\n",
    "        data_batch = data[i:i + len(data) // num_threads]\n",
    "        thread = threading.Thread(target=worker, args=(data_batch,))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "# Function to load data using multiprocessing\n",
    "def load_data_multiprocess(data):\n",
    "    num_processes = 4  # Number of processes to use\n",
    "\n",
    "    def worker(data_batch):\n",
    "        load_data_batch(data_batch)\n",
    "\n",
    "    processes = []\n",
    "    for i in range(0, len(data), len(data) // num_processes):\n",
    "        data_batch = data[i:i + len(data) // num_processes]\n",
    "        process = multiprocessing.Process(target=worker, args=(data_batch,))\n",
    "        processes.append(process)\n",
    "        process.start()\n",
    "\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "# Generate sample data\n",
    "data = [i for i in range(10000)]  # Sample data for loading\n",
    "\n",
    "# Measure loading time before optimization\n",
    "before_optimization_time = measure_loading_time(data)\n",
    "print(\"Loading time before optimization:\", before_optimization_time)\n",
    "\n",
    "# Measure loading time after implementing optimizations\n",
    "after_optimization_time = measure_loading_time(data)\n",
    "print(\"Loading time after optimization:\", after_optimization_time)\n",
    "\n",
    "# Load data using multi-threading\n",
    "load_data_multithread(data)\n",
    "\n",
    "# Load data using multiprocessing\n",
    "load_data_multiprocess(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76089743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
