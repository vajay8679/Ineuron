{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfc5597",
   "metadata": {},
   "source": [
    "# Assignment 3 - (Hadoop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8151045c",
   "metadata": {},
   "source": [
    "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b70c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Read the Hadoop configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('/path/to/hadoop/conf/hadoop-env.sh')\n",
    "\n",
    "# Retrieve the core components from the configuration file\n",
    "core_components = config.get('core-site', 'fs.defaultFS')\n",
    "\n",
    "# Display the core components\n",
    "print(\"Core Components of Hadoop:\")\n",
    "print(core_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c81ec",
   "metadata": {},
   "source": [
    "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf45b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def calculate_total_size(hdfs_url, directory):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # Get a list of files in the directory\n",
    "    file_list = client.list(directory, status=True)\n",
    "\n",
    "    # Calculate the total file size\n",
    "    total_size = 0\n",
    "    for file in file_list:\n",
    "        total_size += file['length']\n",
    "\n",
    "    return total_size\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "hdfs_url = 'http://localhost:9870'\n",
    "directory = '/path/to/hdfs/directory'\n",
    "\n",
    "total_size = calculate_total_size(hdfs_url, directory)\n",
    "print(f\"Total file size in HDFS directory: {total_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05bfed6",
   "metadata": {},
   "source": [
    "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f948e258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N Most Frequent Words:\n",
      "how: 2\n",
      "are: 2\n",
      "you: 2\n",
      "hello: 1\n",
      "doing: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Specify the path to the input file\n",
    "input_file = 'file.txt'\n",
    "\n",
    "# Read the input file and split it into words\n",
    "with open(input_file, 'r') as file:\n",
    "    words = file.read().lower().split()\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Specify the value of N for the top N words\n",
    "top_n = 10\n",
    "\n",
    "# Get the top N most frequent words\n",
    "top_words = word_counts.most_common(top_n)\n",
    "\n",
    "# Display the top N words and their frequencies\n",
    "print(\"Top N Most Frequent Words:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041cfaad",
   "metadata": {},
   "source": [
    "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b356e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Specify the URL of the Hadoop NameNode's web UI\n",
    "namenode_url = 'http://<namenode-host>:<namenode-port>/dfshealth.html'\n",
    "\n",
    "# Specify the URL of the Hadoop DataNode's web UI\n",
    "datanode_url = 'http://<datanode-host>:<datanode-port>/databrowser.html'\n",
    "\n",
    "def check_namenode_health():\n",
    "    response = requests.get(namenode_url)\n",
    "    if response.status_code == 200:\n",
    "        print(\"NameNode is healthy\")\n",
    "    else:\n",
    "        print(\"NameNode is not healthy\")\n",
    "\n",
    "def check_datanode_health():\n",
    "    response = requests.get(datanode_url)\n",
    "    if response.status_code == 200:\n",
    "        print(\"DataNode is healthy\")\n",
    "    else:\n",
    "        print(\"DataNode is not healthy\")\n",
    "\n",
    "# Example usage:\n",
    "check_namenode_health()\n",
    "check_datanode_health()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e916550",
   "metadata": {},
   "source": [
    "5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def list_hdfs_path(hdfs_url, hdfs_path):\n",
    "    client = InsecureClient(hdfs_url)\n",
    "    files = client.list(hdfs_path, status=True)\n",
    "    for file in files:\n",
    "        print(file['path'])\n",
    "\n",
    "# Example usage:\n",
    "hdfs_url = 'http://localhost:9870'\n",
    "hdfs_path = '/path/to/hdfs/directory'\n",
    "\n",
    "list_hdfs_path(hdfs_url, hdfs_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0bd053",
   "metadata": {},
   "source": [
    "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Specify the URL of the Hadoop Namenode's web UI\n",
    "namenode_url = 'http://<namenode-host>:<namenode-port>'\n",
    "\n",
    "def get_datanodes_storage_utilization():\n",
    "    response = requests.get(f\"{namenode_url}/dfshealth.html#tab-datanode\")\n",
    "    if response.status_code == 200:\n",
    "        datanodes_info = response.text\n",
    "        # Parse the response to extract storage utilization information\n",
    "        # and calculate the highest and lowest storage capacities\n",
    "        # Return the highest and lowest storage capacities\n",
    "\n",
    "def analyze_storage_utilization():\n",
    "    storage_utilization = get_datanodes_storage_utilization()\n",
    "\n",
    "    if storage_utilization:\n",
    "        highest_capacity_node = max(storage_utilization, key=storage_utilization.get)\n",
    "        lowest_capacity_node = min(storage_utilization, key=storage_utilization.get)\n",
    "\n",
    "        print(\"Node with the highest storage capacity:\")\n",
    "        print(\"Node:\", highest_capacity_node)\n",
    "        print(\"Capacity:\", storage_utilization[highest_capacity_node])\n",
    "\n",
    "        print(\"Node with the lowest storage capacity:\")\n",
    "        print(\"Node:\", lowest_capacity_node)\n",
    "        print(\"Capacity:\", storage_utilization[lowest_capacity_node])\n",
    "    else:\n",
    "        print(\"Failed to analyze storage utilization\")\n",
    "\n",
    "# Example usage:\n",
    "analyze_storage_utilization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69862497",
   "metadata": {},
   "source": [
    "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af710f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Specify the URL of the YARN ResourceManager's web UI\n",
    "resourcemanager_url = 'http://<resourcemanager-host>:<resourcemanager-port>'\n",
    "\n",
    "def submit_hadoop_job(job_file):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        'application-id': '',\n",
    "        'application-name': 'MyHadoopJob',\n",
    "        'am-container-spec': {\n",
    "            'local-resources': {\n",
    "                'file': {\n",
    "                    'resource': job_file,\n",
    "                    'type': 'FILE',\n",
    "                    'visibility': 'APPLICATION',\n",
    "                }\n",
    "            },\n",
    "            'commands': {\n",
    "                'command': 'hadoop jar ' + job_file,\n",
    "            },\n",
    "            'environment': {},\n",
    "        },\n",
    "        'application-type': 'MAPREDUCE',\n",
    "        'resource': {\n",
    "            'memory': '1024',\n",
    "            'vCores': '1',\n",
    "        },\n",
    "    }\n",
    "\n",
    "    response = requests.post(f\"{resourcemanager_url}/ws/v1/cluster/apps/new-application\", headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        application_id = response.json()['application-id']\n",
    "        data['application-id'] = application_id\n",
    "\n",
    "        response = requests.post(f\"{resourcemanager_url}/ws/v1/cluster/apps\", headers=headers, json=data)\n",
    "        if response.status_code == 202:\n",
    "            print(\"Hadoop job submitted successfully\")\n",
    "            return application_id\n",
    "\n",
    "    print(\"Failed to submit Hadoop job\")\n",
    "    return None\n",
    "\n",
    "def monitor_job_progress(application_id):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    while True:\n",
    "        response = requests.get(f\"{resourcemanager_url}/ws/v1/cluster/apps/{application_id}\", headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            status = response.json()['app']['state']\n",
    "            if status in ['FINISHED', 'FAILED', 'KILLED']:\n",
    "                break\n",
    "            print(\"Job Status:\", status)\n",
    "        else:\n",
    "            print(\"Failed to get job status\")\n",
    "            break\n",
    "        time.sleep(5)\n",
    "\n",
    "def retrieve_job_output(application_id):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response = requests.get(f\"{resourcemanager_url}/proxy/{application_id}/ws/v1/mapreduce/jobs/{application_id}/jobattempts\", headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        job_attempts = response.json()['jobAttempts']\n",
    "        if len(job_attempts) > 0:\n",
    "            last_attempt_id = job_attempts[-1]['id']\n",
    "            response = requests.get(f\"{resourcemanager_url}/proxy/{application_id}/ws/v1/mapreduce/jobs/{application_id}/jobattempts/{last_attempt_id}/counters\", headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                counters = response.json()['jobCounters']['counterGroup'][0]['counter']\n",
    "                for counter in counters:\n",
    "                    if counter['name'] == 'FILE_BYTES_READ':\n",
    "                        print(\"Final Output Size:\", counter['totalCounterValue'])\n",
    "                        break\n",
    "            else:\n",
    "                print(\"Failed to retrieve job counters\")\n",
    "        else:\n",
    "            print(\"No job attempts found\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve job attempts\")\n",
    "\n",
    "# Example usage:\n",
    "job_file = '/path/to/hadoop/job.jar'\n",
    "\n",
    "application_id = submit_hadoop_job(job_file)\n",
    "if application_id:\n",
    "    monitor_job_progress(application_id)\n",
    "    retrieve_job_output(application_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b63456",
   "metadata": {},
   "source": [
    "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17404212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Specify the URL of the YARN ResourceManager's web UI\n",
    "resourcemanager_url = 'http://<resourcemanager-host>:<resourcemanager-port>'\n",
    "\n",
    "def submit_hadoop_job(job_file, memory_mb, vcores):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        'application-id': '',\n",
    "        'application-name': 'MyHadoopJob',\n",
    "        'am-container-spec': {\n",
    "            'local-resources': {\n",
    "                'file': {\n",
    "                    'resource': job_file,\n",
    "                    'type': 'FILE',\n",
    "                    'visibility': 'APPLICATION',\n",
    "                }\n",
    "            },\n",
    "            'commands': {\n",
    "                'command': 'hadoop jar ' + job_file,\n",
    "            },\n",
    "            'environment': {},\n",
    "        },\n",
    "        'application-type': 'MAPREDUCE',\n",
    "        'resource': {\n",
    "            'memory': str(memory_mb),\n",
    "            'vCores': str(vcores),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    response = requests.post(f\"{resourcemanager_url}/ws/v1/cluster/apps/new-application\", headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        application_id = response.json()['application-id']\n",
    "        data['application-id'] = application_id\n",
    "\n",
    "        response = requests.post(f\"{resourcemanager_url}/ws/v1/cluster/apps\", headers=headers, json=data)\n",
    "        if response.status_code == 202:\n",
    "            print(\"Hadoop job submitted successfully\")\n",
    "            return application_id\n",
    "\n",
    "    print(\"Failed to submit Hadoop job\")\n",
    "    return None\n",
    "\n",
    "def track_resource_usage(application_id):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    while True:\n",
    "        response = requests.get(f\"{resourcemanager_url}/ws/v1/cluster/apps/{application_id}/appattempts\", headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            app_attempts = response.json()['appAttempts']\n",
    "            if len(app_attempts) > 0:\n",
    "                last_attempt_id = app_attempts[-1]['appAttemptId']\n",
    "                response = requests.get(f\"{resourcemanager_url}/ws/v1/cluster/apps/{application_id}/appattempts/{last_attempt_id}/containers\", headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    containers = response.json()['containers']\n",
    "                    for container in containers:\n",
    "                        print(\"Container ID:\", container['containerId'])\n",
    "                        print(\"Allocated Memory:\", container['allocatedMB'])\n",
    "                        print(\"Allocated vCores:\", container['allocatedVCores'])\n",
    "                        print(\"\")\n",
    "\n",
    "                        # You can perform further analysis or processing of the container information here\n",
    "\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(\"Failed to retrieve containers\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"No app attempts found\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Failed to get app attempts\")\n",
    "            break\n",
    "\n",
    "# Example usage:\n",
    "job_file = '/path/to/hadoop/job.jar'\n",
    "memory_mb = 1024\n",
    "vcores = 1\n",
    "\n",
    "application_id = submit_hadoop_job(job_file, memory_mb, vcores)\n",
    "if application_id:\n",
    "    track_resource_usage(application_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35359d1d",
   "metadata": {},
   "source": [
    "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9727432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Specify the path to the Hadoop Streaming JAR file\n",
    "hadoop_streaming_jar = \"/path/to/hadoop-streaming.jar\"\n",
    "\n",
    "# Specify the Hadoop input directory and output directory\n",
    "input_dir = \"/path/to/input\"\n",
    "output_dir = \"/path/to/output\"\n",
    "\n",
    "# Specify the different input split sizes to compare\n",
    "input_split_sizes = [\"64\", \"128\", \"256\"]\n",
    "\n",
    "def run_mapreduce_job(input_split_size):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run the Hadoop Streaming job with the specified input split size\n",
    "    cmd = [\n",
    "        \"hadoop\",\n",
    "        \"jar\",\n",
    "        hadoop_streaming_jar,\n",
    "        \"-input\",\n",
    "        input_dir,\n",
    "        \"-output\",\n",
    "        output_dir,\n",
    "        \"-mapper\",\n",
    "        \"mapper.py\",\n",
    "        \"-reducer\",\n",
    "        \"reducer.py\",\n",
    "        \"-inputformat\",\n",
    "        \"org.apache.hadoop.mapred.TextInputFormat\",\n",
    "        \"-jobconf\",\n",
    "        \"mapreduce.input.fileinputformat.split.minsize=\" + input_split_size + \"M\",\n",
    "        \"-jobconf\",\n",
    "        \"mapreduce.input.fileinputformat.split.maxsize=\" + input_split_size + \"M\"\n",
    "    ]\n",
    "    \n",
    "    subprocess.call(cmd)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    return execution_time\n",
    "\n",
    "# Example usage:\n",
    "mapper_script = \"mapper.py\"\n",
    "reducer_script = \"reducer.py\"\n",
    "\n",
    "for split_size in input_split_sizes:\n",
    "    # Prepare and upload the mapper and reducer scripts to Hadoop cluster\n",
    "    subprocess.call([\"hadoop\", \"fs\", \"-put\", mapper_script, input_dir])\n",
    "    subprocess.call([\"hadoop\", \"fs\", \"-put\", reducer_script, input_dir])\n",
    "    \n",
    "    execution_time = run_mapreduce_job(split_size)\n",
    "    \n",
    "    print(f\"Execution Time with Input Split Size {split_size} MB: {execution_time} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
